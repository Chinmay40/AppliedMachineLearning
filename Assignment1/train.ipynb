{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Assignment 1 - Training and Evaluation (MDS202414)"
      ],
      "metadata": {
        "id": "xYxBJoB3VWCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to load the data, drop na and convert spam label to 1 and ham label to 0\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def load_splits(train_path, val_path, test_path, label_col=\"label\", pos_label=\"spam\"):\n",
        "    # Load CSVs\n",
        "    train = pd.read_csv(train_path)\n",
        "    val   = pd.read_csv(val_path)\n",
        "    test  = pd.read_csv(test_path)\n",
        "\n",
        "    # Drop rows with NA anywhere (features or label)\n",
        "    train = train.dropna()\n",
        "    val   = val.dropna()\n",
        "    test  = test.dropna()\n",
        "\n",
        "    # Convert labels to binary\n",
        "    def convert_labels(df):\n",
        "        y = (df[label_col] == pos_label).astype(int)\n",
        "        X = df.drop(columns=[label_col])\n",
        "        return X, y\n",
        "\n",
        "    X_train, y_train = convert_labels(train)\n",
        "    X_val, y_val     = convert_labels(val)\n",
        "    X_test, y_test   = convert_labels(test)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
      ],
      "metadata": {
        "id": "IDOYS0joC6dm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_val, y_val, X_test, y_test = load_splits(\"train_set.csv\",\"validation_set.csv\",\"test_set.csv\")"
      ],
      "metadata": {
        "id": "JFojw1E7H5LG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to train and get score for model\n",
        "\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
        "\n",
        "def train_and_eval(model, X_train, y_train, X_val, y_val):\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Get continuous scores\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_score = model.predict_proba(X_val)[:, 1]\n",
        "    elif hasattr(model, \"decision_function\"):\n",
        "        y_score = model.decision_function(X_val)\n",
        "    else:\n",
        "        y_score = None\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(y_val, y_pred)\n",
        "    }\n",
        "\n",
        "    if y_score is not None:\n",
        "        metrics[\"roc_auc\"] = roc_auc_score(y_val, y_score)\n",
        "        metrics[\"pr_auc\"]  = average_precision_score(y_val, y_score)\n",
        "\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "Z0aDAuYpKxq4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for hyperparameter tuning using the score function used in above function\n",
        "\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "\n",
        "def tune_model(model_class, param_grid, X_train, y_train, X_val, y_val, metric=\"pr_auc\"):\n",
        "    best_score = -np.inf\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    keys, values = zip(*param_grid.items())\n",
        "\n",
        "    for v in product(*values):\n",
        "        params = dict(zip(keys, v))\n",
        "        model = model_class(**params)\n",
        "\n",
        "        metrics = train_and_eval(model, X_train, y_train, X_val, y_val)\n",
        "        score = metrics[metric]\n",
        "\n",
        "        print(f\"{model_class.__name__} params={params} metrics={metrics}\")\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = params\n",
        "            best_model = model\n",
        "\n",
        "    return best_model, best_params, best_score\n"
      ],
      "metadata": {
        "id": "RhS_l2giKyJ7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get score on test data, for final comparison between models\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
        "\n",
        "def evaluate_on_test(model_class, best_params, X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    # Stack train + val\n",
        "    X_train_full = np.vstack([X_train, X_val])\n",
        "    y_train_full = np.concatenate([y_train, y_val])\n",
        "\n",
        "    model = model_class(**best_params)\n",
        "    model.fit(X_train_full, y_train_full)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Get scores for AUC\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_score = model.predict_proba(X_test)[:, 1]\n",
        "    elif hasattr(model, \"decision_function\"):\n",
        "        y_score = model.decision_function(X_test)\n",
        "    else:\n",
        "        y_score = None\n",
        "\n",
        "    results = {\"accuracy\": accuracy_score(y_test, y_pred)}\n",
        "\n",
        "    if y_score is not None:\n",
        "        results[\"roc_auc\"] = roc_auc_score(y_test, y_score)\n",
        "        results[\"pr_auc\"]  = average_precision_score(y_test, y_score)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "QIy53_iAK13d"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up the hyperparameter space for the 3 models\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "models = {\n",
        "    \"LogReg\": (LogisticRegression, {\n",
        "        \"C\": [0.01, 0.1, 1, 10],\n",
        "        \"solver\": [\"liblinear\"],\n",
        "        \"max_iter\": [1000]\n",
        "    }),\n",
        "\n",
        "    \"NaiveBayes\": (MultinomialNB, {\n",
        "        \"alpha\": [0.1, 0.5, 1, 2]\n",
        "    }),\n",
        "\n",
        "    \"LinearSVM\": (LinearSVC, {\n",
        "        \"C\": [0.01, 0.1, 1, 10],\n",
        "        \"max_iter\": [5000]\n",
        "    })\n",
        "}\n"
      ],
      "metadata": {
        "id": "IVCFxh5ZK5lP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scaling data -> converting raw counts to proportions\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val   = scaler.transform(X_val)\n",
        "X_test  = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "o2G998eEMe-t"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding best hyper-parameters\n",
        "results = {}\n",
        "\n",
        "for name, (model_class, param_grid) in models.items():\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"TUNING:\", name)\n",
        "\n",
        "    best_model, best_params, best_score = tune_model(\n",
        "        model_class, param_grid,\n",
        "        X_train, y_train, X_val, y_val,\n",
        "        metric=\"pr_auc\"   # spam â†’ PR-AUC best\n",
        "    )\n",
        "\n",
        "    print(f\"BEST {name}: params={best_params}, val_score={best_score}\")\n",
        "\n",
        "    test_metrics = evaluate_on_test(\n",
        "        model_class, best_params,\n",
        "        X_train, y_train, X_val, y_val, X_test, y_test\n",
        "    )\n",
        "\n",
        "    results[name] = {\"best_params\": best_params, \"test_metrics\": test_metrics}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjpKV6qSK8ZZ",
        "outputId": "f6b90dd6-d387-44fc-a702-e983ebb14d24"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "TUNING: LogReg\n",
            "LogisticRegression params={'C': 0.01, 'solver': 'liblinear', 'max_iter': 1000} metrics={'accuracy': 0.972488038277512, 'roc_auc': np.float64(0.9609720798408008), 'pr_auc': np.float64(0.9337120656590089)}\n",
            "LogisticRegression params={'C': 0.1, 'solver': 'liblinear', 'max_iter': 1000} metrics={'accuracy': 0.9712918660287081, 'roc_auc': np.float64(0.9605861424350238), 'pr_auc': np.float64(0.933600426169063)}\n",
            "LogisticRegression params={'C': 1, 'solver': 'liblinear', 'max_iter': 1000} metrics={'accuracy': 0.972488038277512, 'roc_auc': np.float64(0.9606826267864682), 'pr_auc': np.float64(0.9351292102410739)}\n",
            "LogisticRegression params={'C': 10, 'solver': 'liblinear', 'max_iter': 1000} metrics={'accuracy': 0.9712918660287081, 'roc_auc': np.float64(0.9622504974974371), 'pr_auc': np.float64(0.9360148391560262)}\n",
            "BEST LogReg: params={'C': 10, 'solver': 'liblinear', 'max_iter': 1000}, val_score=0.9360148391560262\n",
            "\n",
            "==============================\n",
            "TUNING: NaiveBayes\n",
            "MultinomialNB params={'alpha': 0.1} metrics={'accuracy': 0.9509569377990431, 'roc_auc': np.float64(0.9631128263884702), 'pr_auc': np.float64(0.848404737673039)}\n",
            "MultinomialNB params={'alpha': 0.5} metrics={'accuracy': 0.9557416267942583, 'roc_auc': np.float64(0.9645480311162034), 'pr_auc': np.float64(0.8415999163142216)}\n",
            "MultinomialNB params={'alpha': 1} metrics={'accuracy': 0.9557416267942583, 'roc_auc': np.float64(0.9663631429777484), 'pr_auc': np.float64(0.8445417519805075)}\n",
            "MultinomialNB params={'alpha': 2} metrics={'accuracy': 0.9509569377990431, 'roc_auc': np.float64(0.968196345655189), 'pr_auc': np.float64(0.8473034540603926)}\n",
            "BEST NaiveBayes: params={'alpha': 0.1}, val_score=0.848404737673039\n",
            "\n",
            "==============================\n",
            "TUNING: LinearSVM\n",
            "LinearSVC params={'C': 0.01, 'max_iter': 5000} metrics={'accuracy': 0.972488038277512, 'roc_auc': np.float64(0.9558222275824639), 'pr_auc': np.float64(0.929371472215542)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC params={'C': 0.1, 'max_iter': 5000} metrics={'accuracy': 0.9712918660287081, 'roc_auc': np.float64(0.9626967376228668), 'pr_auc': np.float64(0.9355868138749771)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC params={'C': 1, 'max_iter': 5000} metrics={'accuracy': 0.965311004784689, 'roc_auc': np.float64(0.9840439003799071), 'pr_auc': np.float64(0.9664385381908788)}\n",
            "LinearSVC params={'C': 10, 'max_iter': 5000} metrics={'accuracy': 0.9665071770334929, 'roc_auc': np.float64(0.9888801784960501), 'pr_auc': np.float64(0.9743866213819193)}\n",
            "BEST LinearSVM: params={'C': 10, 'max_iter': 5000}, val_score=0.9743866213819193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluating on test data to compare the 3 models\n",
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame({\n",
        "    name: res[\"test_metrics\"] for name, res in results.items()\n",
        "}).T\n",
        "\n",
        "print(df_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nui0XfVHLs75",
        "outputId": "17ff7b09-879b-4440-fac0-505b698f4120"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            accuracy   roc_auc    pr_auc\n",
            "LogReg      0.976105  0.978362  0.959598\n",
            "NaiveBayes  0.958184  0.973498  0.875465\n",
            "LinearSVM   0.972521  0.989101  0.947018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results suggest:\n",
        "1. Logistic Regression gives best results overall.\n",
        "2. LinearSVM has low PR-AUC but high ROC-AUC, which suggests it has many false positives.\n",
        "3. NaiveBayes seems to eb overfitting on train / validation dataset, leading to poor accuracy and PR-AUC."
      ],
      "metadata": {
        "id": "gSiZkyxyQ1ZW"
      }
    }
  ]
}
