{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Assignment 2 - Model Version Control (MDS202414)"
      ],
      "metadata": {
        "id": "xYxBJoB3VWCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install mlflow"
      ],
      "metadata": {
        "id": "rSQ_rGLGGRyU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow"
      ],
      "metadata": {
        "id": "tG1uk7drGTrB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading splits"
      ],
      "metadata": {
        "id": "nMt3l28qe1IW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to load the data, drop na and convert spam label to 1 and ham label to 0\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def load_splits(train_path, val_path, test_path, label_col=\"label\", pos_label=\"spam\"):\n",
        "    # Load CSVs\n",
        "    train = pd.read_csv(train_path)\n",
        "    val   = pd.read_csv(val_path)\n",
        "    test  = pd.read_csv(test_path)\n",
        "\n",
        "    # Drop rows with NA anywhere (features or label)\n",
        "    train = train.dropna()\n",
        "    val   = val.dropna()\n",
        "    test  = test.dropna()\n",
        "\n",
        "    # Convert labels to binary\n",
        "    def convert_labels(df):\n",
        "        y = (df[label_col] == pos_label).astype(int)\n",
        "        X = df.drop(columns=[label_col])\n",
        "        return X, y\n",
        "\n",
        "    X_train, y_train = convert_labels(train)\n",
        "    X_val, y_val     = convert_labels(val)\n",
        "    X_test, y_test   = convert_labels(test)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
      ],
      "metadata": {
        "id": "IDOYS0joC6dm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_val, y_val, X_test, y_test = load_splits(\"train_set.csv\",\"validation_set.csv\",\"test_set.csv\")"
      ],
      "metadata": {
        "id": "JFojw1E7H5LG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scaling data -> converting raw counts to proportions\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val   = scaler.transform(X_val)\n",
        "X_test  = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "o2G998eEMe-t"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ML pipeline"
      ],
      "metadata": {
        "id": "H1tHqy-Ne4_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to train and get score for model\n",
        "\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
        "\n",
        "def train_and_eval(model, X_train, y_train, X_val, y_val):\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Get continuous scores\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_score = model.predict_proba(X_val)[:, 1]\n",
        "    elif hasattr(model, \"decision_function\"):\n",
        "        y_score = model.decision_function(X_val)\n",
        "    else:\n",
        "        y_score = None\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(y_val, y_pred)\n",
        "    }\n",
        "\n",
        "    if y_score is not None:\n",
        "        metrics[\"roc_auc\"] = roc_auc_score(y_val, y_score)\n",
        "        metrics[\"pr_auc\"]  = average_precision_score(y_val, y_score)\n",
        "\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "Z0aDAuYpKxq4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for hyperparameter tuning using the score function used in above function\n",
        "\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "\n",
        "def tune_model(model_class, param_grid, X_train, y_train, X_val, y_val, metric=\"pr_auc\"):\n",
        "    best_score = -np.inf\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    keys, values = zip(*param_grid.items())\n",
        "\n",
        "    for v in product(*values):\n",
        "        params = dict(zip(keys, v))\n",
        "        model = model_class(**params)\n",
        "\n",
        "        metrics = train_and_eval(model, X_train, y_train, X_val, y_val)\n",
        "        score = metrics[metric]\n",
        "\n",
        "        print(f\"{model_class.__name__} params={params} metrics={metrics}\")\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = params\n",
        "            best_model = model\n",
        "\n",
        "    return best_model, best_params, best_score\n"
      ],
      "metadata": {
        "id": "RhS_l2giKyJ7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up the hyperparameter space for the 3 models\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "models = {\n",
        "    \"LogReg\": (LogisticRegression, {\n",
        "        \"C\": [0.01, 0.1, 1, 10],\n",
        "        \"solver\": [\"liblinear\"],\n",
        "        \"max_iter\": [1000]\n",
        "    }),\n",
        "\n",
        "    \"NaiveBayes\": (MultinomialNB, {\n",
        "        \"alpha\": [0.1, 0.5, 1, 2]\n",
        "    }),\n",
        "\n",
        "    \"LinearSVM\": (LinearSVC, {\n",
        "        \"C\": [0.01, 0.1, 1, 10],\n",
        "        \"max_iter\": [5000]\n",
        "    })\n",
        "}\n"
      ],
      "metadata": {
        "id": "IVCFxh5ZK5lP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Function for logging experiment along with evaluation of model on test data"
      ],
      "metadata": {
        "id": "inU1Hw1MfHLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model_and_versioning(model_name, model_class, best_params,\n",
        "                             X_train, y_train, X_val, y_val,\n",
        "                             X_test, y_test):\n",
        "\n",
        "    with mlflow.start_run(run_name=model_name):\n",
        "\n",
        "        # Combine train + val\n",
        "        X_train_full = np.vstack([X_train, X_val])\n",
        "        y_train_full = np.concatenate([y_train, y_val])\n",
        "\n",
        "        model = model_class(**best_params)\n",
        "        model.fit(X_train_full, y_train_full)\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Scores\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            y_score = model.predict_proba(X_test)[:, 1]\n",
        "        elif hasattr(model, \"decision_function\"):\n",
        "            y_score = model.decision_function(X_test)\n",
        "        else:\n",
        "            y_score = None\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        roc_auc = roc_auc_score(y_test, y_score)\n",
        "        pr_auc  = average_precision_score(y_test, y_score)\n",
        "\n",
        "        # Log params\n",
        "        mlflow.log_params(best_params)\n",
        "\n",
        "        # Log metrics\n",
        "        mlflow.log_metric(\"accuracy\", accuracy)\n",
        "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
        "        mlflow.log_metric(\"pr_auc\", pr_auc)\n",
        "\n",
        "        # Log model\n",
        "        mlflow.sklearn.log_model(model, \"model\")\n",
        "\n",
        "        run_id = mlflow.active_run().info.run_id\n",
        "\n",
        "    return run_id, {\"accuracy\": accuracy,\n",
        "                    \"roc_auc\": roc_auc,\n",
        "                    \"pr_auc\": pr_auc}\n"
      ],
      "metadata": {
        "id": "pHsVScFuHeCs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Executing the runs for each model"
      ],
      "metadata": {
        "id": "ZXk6vo5CfS1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.set_experiment(\"MLflow Assignment2\")\n",
        "\n",
        "results={}\n",
        "\n",
        "for name, (model_class, param_grid) in models.items():\n",
        "\n",
        "    best_model, best_params, best_score =  tune_model(\n",
        "        model_class, param_grid,\n",
        "        X_train, y_train, X_val, y_val,\n",
        "        metric=\"pr_auc\"   # spam â†’ PR-AUC best\n",
        "    )\n",
        "\n",
        "    run_id, test_metrics = run_model_and_versioning(\n",
        "        name, model_class, best_params,\n",
        "        X_train, y_train, X_val, y_val,\n",
        "        X_test, y_test\n",
        "    )\n",
        "\n",
        "    # Register model\n",
        "    model_uri = f\"runs:/{run_id}/model\"\n",
        "\n",
        "    mlflow.register_model(\n",
        "        model_uri=model_uri,\n",
        "        name=\"BenchmarkModels\"\n",
        "    )\n",
        "\n",
        "    results[name] = {\n",
        "        \"best_params\": best_params,\n",
        "        \"test_metrics\": test_metrics\n",
        "    }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGNxemcMJGHu",
        "outputId": "48a3f32b-3b7f-48ef-e51c-363fb9c4b552"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026/02/15 18:15:16 INFO mlflow.tracking.fluent: Experiment with name 'MLflow Assignment2' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression params={'C': 0.01, 'solver': 'liblinear', 'max_iter': 1000} metrics={'accuracy': 0.972488038277512, 'roc_auc': np.float64(0.9609720798408008), 'pr_auc': np.float64(0.9337120656590089)}\n",
            "LogisticRegression params={'C': 0.1, 'solver': 'liblinear', 'max_iter': 1000} metrics={'accuracy': 0.9712918660287081, 'roc_auc': np.float64(0.9605861424350238), 'pr_auc': np.float64(0.933600426169063)}\n",
            "LogisticRegression params={'C': 1, 'solver': 'liblinear', 'max_iter': 1000} metrics={'accuracy': 0.972488038277512, 'roc_auc': np.float64(0.9606826267864682), 'pr_auc': np.float64(0.9351292102410739)}\n",
            "LogisticRegression params={'C': 10, 'solver': 'liblinear', 'max_iter': 1000} metrics={'accuracy': 0.9712918660287081, 'roc_auc': np.float64(0.9622504974974371), 'pr_auc': np.float64(0.9360148391560262)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026/02/15 18:15:20 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/mlflow/models/model.py:1209: FutureWarning: Saving scikit-learn models in the pickle or cloudpickle format requires exercising caution because these formats rely on Python's object serialization mechanism, which can execute arbitrary code during deserialization.The recommended safe alternative is the 'skops' format.\n",
            "  flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n",
            "Successfully registered model 'BenchmarkModels'.\n",
            "2026/02/15 18:15:25 WARNING mlflow.tracking._model_registry.fluent: Run with id 05beeb36e290470bbfefab296d92181f has no artifacts at artifact path 'model', registering model based on models:/m-cb1aead0dd034606976239be890f522a instead\n",
            "Created version '1' of model 'BenchmarkModels'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultinomialNB params={'alpha': 0.1} metrics={'accuracy': 0.9509569377990431, 'roc_auc': np.float64(0.9631128263884702), 'pr_auc': np.float64(0.848404737673039)}\n",
            "MultinomialNB params={'alpha': 0.5} metrics={'accuracy': 0.9557416267942583, 'roc_auc': np.float64(0.9645480311162034), 'pr_auc': np.float64(0.8415999163142216)}\n",
            "MultinomialNB params={'alpha': 1} metrics={'accuracy': 0.9557416267942583, 'roc_auc': np.float64(0.9663631429777484), 'pr_auc': np.float64(0.8445417519805075)}\n",
            "MultinomialNB params={'alpha': 2} metrics={'accuracy': 0.9509569377990431, 'roc_auc': np.float64(0.968196345655189), 'pr_auc': np.float64(0.8473034540603926)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026/02/15 18:15:27 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/mlflow/models/model.py:1209: FutureWarning: Saving scikit-learn models in the pickle or cloudpickle format requires exercising caution because these formats rely on Python's object serialization mechanism, which can execute arbitrary code during deserialization.The recommended safe alternative is the 'skops' format.\n",
            "  flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n",
            "Registered model 'BenchmarkModels' already exists. Creating a new version of this model...\n",
            "2026/02/15 18:15:34 WARNING mlflow.tracking._model_registry.fluent: Run with id 7d045f5d5c3c47f7a1eac63caa67ce8d has no artifacts at artifact path 'model', registering model based on models:/m-2dcd74c581d04773900568cf225c47a1 instead\n",
            "Created version '2' of model 'BenchmarkModels'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC params={'C': 0.01, 'max_iter': 5000} metrics={'accuracy': 0.972488038277512, 'roc_auc': np.float64(0.9558222275824639), 'pr_auc': np.float64(0.929371472215542)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC params={'C': 0.1, 'max_iter': 5000} metrics={'accuracy': 0.9712918660287081, 'roc_auc': np.float64(0.9626967376228668), 'pr_auc': np.float64(0.9355868138749771)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC params={'C': 1, 'max_iter': 5000} metrics={'accuracy': 0.965311004784689, 'roc_auc': np.float64(0.9840439003799071), 'pr_auc': np.float64(0.9664385381908788)}\n",
            "LinearSVC params={'C': 10, 'max_iter': 5000} metrics={'accuracy': 0.9665071770334929, 'roc_auc': np.float64(0.9888801784960501), 'pr_auc': np.float64(0.9743866213819193)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026/02/15 18:15:40 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/mlflow/models/model.py:1209: FutureWarning: Saving scikit-learn models in the pickle or cloudpickle format requires exercising caution because these formats rely on Python's object serialization mechanism, which can execute arbitrary code during deserialization.The recommended safe alternative is the 'skops' format.\n",
            "  flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n",
            "Registered model 'BenchmarkModels' already exists. Creating a new version of this model...\n",
            "2026/02/15 18:15:45 WARNING mlflow.tracking._model_registry.fluent: Run with id d88d4647745648719f1c389a6840a9c6 has no artifacts at artifact path 'model', registering model based on models:/m-df753465777146db93994ca848c7953b instead\n",
            "Created version '3' of model 'BenchmarkModels'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluating on test data to compare the 3 models\n",
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame({\n",
        "    name: res[\"test_metrics\"] for name, res in results.items()\n",
        "}).T\n",
        "\n",
        "print(df_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nui0XfVHLs75",
        "outputId": "df5d52d8-28cf-4b1a-9e24-630f557917cf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            accuracy   roc_auc    pr_auc\n",
            "LogReg      0.976105  0.978362  0.959598\n",
            "NaiveBayes  0.958184  0.973498  0.875465\n",
            "LinearSVM   0.972521  0.989101  0.947007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Checkout and printing AUCPR"
      ],
      "metadata": {
        "id": "-R907Zh3f32i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "\n",
        "# Get experiment by name\n",
        "experiment = mlflow.get_experiment_by_name(\"MLflow Assignment2\")\n",
        "\n",
        "# Search all runs in that experiment\n",
        "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
        "\n",
        "for _, row in runs.iterrows():\n",
        "    run_id = row[\"run_id\"]\n",
        "    model_name = row.get(\"tags.mlflow.runName\", \"Unknown\")\n",
        "    pr_auc = row.get(\"metrics.pr_auc\", None)\n",
        "\n",
        "    print(f\"Run: {model_name} | Run ID: {run_id} | AUCPR: {pr_auc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke1FlumyJhR0",
        "outputId": "af0725aa-c658-446e-a16b-0081cfba8744"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: LinearSVM | Run ID: d88d4647745648719f1c389a6840a9c6 | AUCPR: 0.947006747962271\n",
            "Run: NaiveBayes | Run ID: 7d045f5d5c3c47f7a1eac63caa67ce8d | AUCPR: 0.8754648199110368\n",
            "Run: LogReg | Run ID: 05beeb36e290470bbfefab296d92181f | AUCPR: 0.959597646859366\n"
          ]
        }
      ]
    }
  ]
}